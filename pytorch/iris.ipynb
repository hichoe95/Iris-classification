{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3ef66b07d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from __future__ import print_function\n",
    "import os.path\n",
    "import torch.nn.parallel\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import random\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/iris.txt', 'r').readlines()\n",
    "\n",
    "N = len(f) - 1\n",
    "iris = []\n",
    "\n",
    "for i in range(0,N):\n",
    "    line = f[i].split(',')\n",
    "    iris.append(line)\n",
    "\n",
    "random.shuffle(iris)\n",
    "\n",
    "# print(iris)\n",
    "\n",
    "iris = np.asarray(iris)\n",
    "\n",
    "feature = iris[:,:-1]\n",
    "feature = feature.astype(np.float)\n",
    "\n",
    "label = iris[:,-1]\n",
    "\n",
    "dic = {'Iris-setosa' : 0, 'Iris-versicolor' : 1, 'Iris-virginica' : 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide train data and test data.\n",
    "train_len = int(len(feature)*0.7)\n",
    "\n",
    "train_feature = feature[:train_len]\n",
    "test_feature = feature[train_len:]\n",
    "\n",
    "train_label = label[:train_len]\n",
    "test_label = label[train_len:]\n",
    "# print(train_feature)\n",
    "# print(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(train_feature).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model,self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        cur_dim = 16\n",
    "        \n",
    "        layers.append(nn.Linear(4,cur_dim))\n",
    "        \n",
    "        for i in range(0,2):\n",
    "            layers.append(nn.Linear(cur_dim,cur_dim//2))\n",
    "            layers.append(nn.ReLU(inplace = True))\n",
    "            cur_dim = cur_dim//2\n",
    "        \n",
    "        # make probability for each class\n",
    "        layers.append(nn.Linear(cur_dim,3))\n",
    "#         layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.main = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = model()\n",
    "\n",
    "# use crossentropyloss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# use gradient descent optimizer, and apply lr = 0.1\n",
    "optimizer = torch.optim.SGD(nets.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/ipykernel_launcher.py:27: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 , loss : 0.901460, accuracy : 100.000000\n",
      "epoch : 1 , loss : 0.976810, accuracy : 35.294118\n",
      "epoch : 1 , loss : 1.101259, accuracy : 38.613861\n",
      "epoch : 2 , loss : 0.957404, accuracy : 100.000000\n",
      "epoch : 2 , loss : 0.979549, accuracy : 35.294118\n",
      "epoch : 2 , loss : 1.130689, accuracy : 38.613861\n",
      "epoch : 3 , loss : 0.947159, accuracy : 100.000000\n",
      "epoch : 3 , loss : 0.971764, accuracy : 35.294118\n",
      "epoch : 3 , loss : 1.151249, accuracy : 38.613861\n",
      "epoch : 4 , loss : 0.935331, accuracy : 100.000000\n",
      "epoch : 4 , loss : 0.960981, accuracy : 35.294118\n",
      "epoch : 4 , loss : 1.163885, accuracy : 38.613861\n",
      "epoch : 5 , loss : 0.915331, accuracy : 100.000000\n",
      "epoch : 5 , loss : 0.935357, accuracy : 35.294118\n",
      "epoch : 5 , loss : 1.168208, accuracy : 38.613861\n",
      "epoch : 6 , loss : 0.866212, accuracy : 100.000000\n",
      "epoch : 6 , loss : 0.875531, accuracy : 35.294118\n",
      "epoch : 6 , loss : 1.161340, accuracy : 38.613861\n",
      "epoch : 7 , loss : 0.748357, accuracy : 100.000000\n",
      "epoch : 7 , loss : 0.720592, accuracy : 35.294118\n",
      "epoch : 7 , loss : 1.131493, accuracy : 38.613861\n",
      "epoch : 8 , loss : 0.480111, accuracy : 100.000000\n",
      "epoch : 8 , loss : 0.443327, accuracy : 43.137255\n",
      "epoch : 8 , loss : 1.075244, accuracy : 50.495050\n",
      "epoch : 9 , loss : 0.251395, accuracy : 100.000000\n",
      "epoch : 9 , loss : 0.180162, accuracy : 56.862745\n",
      "epoch : 9 , loss : 1.016653, accuracy : 61.386139\n",
      "epoch : 10 , loss : 0.132093, accuracy : 100.000000\n",
      "epoch : 10 , loss : 0.091735, accuracy : 56.862745\n",
      "epoch : 10 , loss : 0.969640, accuracy : 64.356436\n",
      "epoch : 11 , loss : 0.083870, accuracy : 100.000000\n",
      "epoch : 11 , loss : 0.056437, accuracy : 60.784314\n",
      "epoch : 11 , loss : 0.934498, accuracy : 67.326733\n",
      "epoch : 12 , loss : 0.053329, accuracy : 100.000000\n",
      "epoch : 12 , loss : 0.037420, accuracy : 62.745098\n",
      "epoch : 12 , loss : 0.907717, accuracy : 68.316832\n",
      "epoch : 13 , loss : 0.036843, accuracy : 100.000000\n",
      "epoch : 13 , loss : 0.025709, accuracy : 62.745098\n",
      "epoch : 13 , loss : 0.886773, accuracy : 68.316832\n",
      "epoch : 14 , loss : 0.025977, accuracy : 100.000000\n",
      "epoch : 14 , loss : 0.018272, accuracy : 62.745098\n",
      "epoch : 14 , loss : 0.870243, accuracy : 68.316832\n",
      "epoch : 15 , loss : 0.018820, accuracy : 100.000000\n",
      "epoch : 15 , loss : 0.009248, accuracy : 62.745098\n",
      "epoch : 15 , loss : 0.856693, accuracy : 68.316832\n",
      "epoch : 16 , loss : 0.014383, accuracy : 100.000000\n",
      "epoch : 16 , loss : 0.007971, accuracy : 64.705882\n",
      "epoch : 16 , loss : 0.845489, accuracy : 69.306931\n",
      "epoch : 17 , loss : 0.011279, accuracy : 100.000000\n",
      "epoch : 17 , loss : 0.006932, accuracy : 64.705882\n",
      "epoch : 17 , loss : 0.836048, accuracy : 69.306931\n",
      "epoch : 18 , loss : 0.009042, accuracy : 100.000000\n",
      "epoch : 18 , loss : 0.006138, accuracy : 64.705882\n",
      "epoch : 18 , loss : 0.827981, accuracy : 69.306931\n",
      "epoch : 19 , loss : 0.007429, accuracy : 100.000000\n",
      "epoch : 19 , loss : 0.005471, accuracy : 64.705882\n",
      "epoch : 19 , loss : 0.820996, accuracy : 69.306931\n",
      "epoch : 20 , loss : 0.006379, accuracy : 100.000000\n",
      "epoch : 20 , loss : 0.004898, accuracy : 64.705882\n",
      "epoch : 20 , loss : 0.814884, accuracy : 69.306931\n",
      "epoch : 21 , loss : 0.005670, accuracy : 100.000000\n",
      "epoch : 21 , loss : 0.004478, accuracy : 64.705882\n",
      "epoch : 21 , loss : 0.809411, accuracy : 69.306931\n",
      "epoch : 22 , loss : 0.005001, accuracy : 100.000000\n",
      "epoch : 22 , loss : 0.004032, accuracy : 64.705882\n",
      "epoch : 22 , loss : 0.804543, accuracy : 69.306931\n",
      "epoch : 23 , loss : 0.004470, accuracy : 100.000000\n",
      "epoch : 23 , loss : 0.003692, accuracy : 64.705882\n",
      "epoch : 23 , loss : 0.800179, accuracy : 69.306931\n",
      "epoch : 24 , loss : 0.004046, accuracy : 100.000000\n",
      "epoch : 24 , loss : 0.003416, accuracy : 64.705882\n",
      "epoch : 24 , loss : 0.796226, accuracy : 69.306931\n",
      "epoch : 25 , loss : 0.003672, accuracy : 100.000000\n",
      "epoch : 25 , loss : 0.003117, accuracy : 64.705882\n",
      "epoch : 25 , loss : 0.792624, accuracy : 69.306931\n",
      "epoch : 26 , loss : 0.003370, accuracy : 100.000000\n",
      "epoch : 26 , loss : 0.002858, accuracy : 64.705882\n",
      "epoch : 26 , loss : 0.789218, accuracy : 69.306931\n",
      "epoch : 27 , loss : 0.003342, accuracy : 100.000000\n",
      "epoch : 27 , loss : 0.001829, accuracy : 64.705882\n",
      "epoch : 27 , loss : 0.782348, accuracy : 69.306931\n",
      "epoch : 28 , loss : 0.003487, accuracy : 100.000000\n",
      "epoch : 28 , loss : 0.002151, accuracy : 64.705882\n",
      "epoch : 28 , loss : 0.768507, accuracy : 69.306931\n",
      "epoch : 29 , loss : 0.003395, accuracy : 100.000000\n",
      "epoch : 29 , loss : 0.002210, accuracy : 66.666667\n",
      "epoch : 29 , loss : 0.749564, accuracy : 70.297030\n",
      "epoch : 30 , loss : 0.003112, accuracy : 100.000000\n",
      "epoch : 30 , loss : 0.001861, accuracy : 80.392157\n",
      "epoch : 30 , loss : 0.724488, accuracy : 81.188119\n",
      "epoch : 31 , loss : 0.003269, accuracy : 100.000000\n",
      "epoch : 31 , loss : 0.002390, accuracy : 88.235294\n",
      "epoch : 31 , loss : 0.690691, accuracy : 86.138614\n",
      "epoch : 32 , loss : 0.005149, accuracy : 100.000000\n",
      "epoch : 32 , loss : 0.002989, accuracy : 88.235294\n",
      "epoch : 32 , loss : 0.656646, accuracy : 89.108911\n",
      "epoch : 33 , loss : 0.005081, accuracy : 100.000000\n",
      "epoch : 33 , loss : 0.003741, accuracy : 92.156863\n",
      "epoch : 33 , loss : 0.615329, accuracy : 91.089109\n",
      "epoch : 34 , loss : 0.005686, accuracy : 100.000000\n",
      "epoch : 34 , loss : 0.004313, accuracy : 88.235294\n",
      "epoch : 34 , loss : 0.568110, accuracy : 87.128713\n",
      "epoch : 35 , loss : 0.006156, accuracy : 100.000000\n",
      "epoch : 35 , loss : 0.004527, accuracy : 88.235294\n",
      "epoch : 35 , loss : 0.524483, accuracy : 88.118812\n",
      "epoch : 36 , loss : 0.006773, accuracy : 100.000000\n",
      "epoch : 36 , loss : 0.004730, accuracy : 90.196078\n",
      "epoch : 36 , loss : 0.482269, accuracy : 88.118812\n",
      "epoch : 37 , loss : 0.007363, accuracy : 100.000000\n",
      "epoch : 37 , loss : 0.005059, accuracy : 92.156863\n",
      "epoch : 37 , loss : 0.443791, accuracy : 91.089109\n",
      "epoch : 38 , loss : 0.004241, accuracy : 100.000000\n",
      "epoch : 38 , loss : 0.004057, accuracy : 96.078431\n",
      "epoch : 38 , loss : 0.403708, accuracy : 92.079208\n",
      "epoch : 39 , loss : 0.004231, accuracy : 100.000000\n",
      "epoch : 39 , loss : 0.004150, accuracy : 96.078431\n",
      "epoch : 39 , loss : 0.369748, accuracy : 92.079208\n",
      "epoch : 40 , loss : 0.004222, accuracy : 100.000000\n",
      "epoch : 40 , loss : 0.004580, accuracy : 98.039216\n",
      "epoch : 40 , loss : 0.340736, accuracy : 93.069307\n",
      "epoch : 41 , loss : 0.004226, accuracy : 100.000000\n",
      "epoch : 41 , loss : 0.004597, accuracy : 98.039216\n",
      "epoch : 41 , loss : 0.314576, accuracy : 94.059406\n",
      "epoch : 42 , loss : 0.006247, accuracy : 100.000000\n",
      "epoch : 42 , loss : 0.005055, accuracy : 98.039216\n",
      "epoch : 42 , loss : 0.291503, accuracy : 93.069307\n",
      "epoch : 43 , loss : 0.006184, accuracy : 100.000000\n",
      "epoch : 43 , loss : 0.005116, accuracy : 98.039216\n",
      "epoch : 43 , loss : 0.272608, accuracy : 93.069307\n",
      "epoch : 44 , loss : 0.005986, accuracy : 100.000000\n",
      "epoch : 44 , loss : 0.008902, accuracy : 98.039216\n",
      "epoch : 44 , loss : 0.255202, accuracy : 93.069307\n",
      "epoch : 45 , loss : 0.006209, accuracy : 100.000000\n",
      "epoch : 45 , loss : 0.008318, accuracy : 100.000000\n",
      "epoch : 45 , loss : 0.239769, accuracy : 94.059406\n",
      "epoch : 46 , loss : 0.005627, accuracy : 100.000000\n",
      "epoch : 46 , loss : 0.007779, accuracy : 98.039216\n",
      "epoch : 46 , loss : 0.226104, accuracy : 93.069307\n",
      "epoch : 47 , loss : 0.005097, accuracy : 100.000000\n",
      "epoch : 47 , loss : 0.006357, accuracy : 98.039216\n",
      "epoch : 47 , loss : 0.213852, accuracy : 95.049505\n",
      "epoch : 48 , loss : 0.004952, accuracy : 100.000000\n",
      "epoch : 48 , loss : 0.006067, accuracy : 98.039216\n",
      "epoch : 48 , loss : 0.202711, accuracy : 96.039604\n",
      "epoch : 49 , loss : 0.004908, accuracy : 100.000000\n",
      "epoch : 49 , loss : 0.005950, accuracy : 98.039216\n",
      "epoch : 49 , loss : 0.192644, accuracy : 95.049505\n",
      "epoch : 50 , loss : 0.005052, accuracy : 100.000000\n",
      "epoch : 50 , loss : 0.005874, accuracy : 98.039216\n",
      "epoch : 50 , loss : 0.183703, accuracy : 95.049505\n",
      "epoch : 51 , loss : 0.005029, accuracy : 100.000000\n",
      "epoch : 51 , loss : 0.005595, accuracy : 98.039216\n",
      "epoch : 51 , loss : 0.176242, accuracy : 95.049505\n",
      "epoch : 52 , loss : 0.005051, accuracy : 100.000000\n",
      "epoch : 52 , loss : 0.005451, accuracy : 98.039216\n",
      "epoch : 52 , loss : 0.169332, accuracy : 95.049505\n",
      "epoch : 53 , loss : 0.004800, accuracy : 100.000000\n",
      "epoch : 53 , loss : 0.005332, accuracy : 98.039216\n",
      "epoch : 53 , loss : 0.163066, accuracy : 95.049505\n",
      "epoch : 54 , loss : 0.004461, accuracy : 100.000000\n",
      "epoch : 54 , loss : 0.005155, accuracy : 98.039216\n",
      "epoch : 54 , loss : 0.157327, accuracy : 94.059406\n",
      "epoch : 55 , loss : 0.004143, accuracy : 100.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 55 , loss : 0.005131, accuracy : 98.039216\n",
      "epoch : 55 , loss : 0.152001, accuracy : 94.059406\n",
      "epoch : 56 , loss : 0.003853, accuracy : 100.000000\n",
      "epoch : 56 , loss : 0.004967, accuracy : 98.039216\n",
      "epoch : 56 , loss : 0.147093, accuracy : 94.059406\n",
      "epoch : 57 , loss : 0.003501, accuracy : 100.000000\n",
      "epoch : 57 , loss : 0.004787, accuracy : 98.039216\n",
      "epoch : 57 , loss : 0.142437, accuracy : 93.069307\n",
      "epoch : 58 , loss : 0.003078, accuracy : 100.000000\n",
      "epoch : 58 , loss : 0.005056, accuracy : 100.000000\n",
      "epoch : 58 , loss : 0.137991, accuracy : 94.059406\n",
      "epoch : 59 , loss : 0.002035, accuracy : 100.000000\n",
      "epoch : 59 , loss : 0.005508, accuracy : 100.000000\n",
      "epoch : 59 , loss : 0.132642, accuracy : 96.039604\n",
      "epoch : 60 , loss : 0.001960, accuracy : 100.000000\n",
      "epoch : 60 , loss : 0.005252, accuracy : 100.000000\n",
      "epoch : 60 , loss : 0.127925, accuracy : 95.049505\n",
      "epoch : 61 , loss : 0.002032, accuracy : 100.000000\n",
      "epoch : 61 , loss : 0.004470, accuracy : 100.000000\n",
      "epoch : 61 , loss : 0.123014, accuracy : 95.049505\n",
      "epoch : 62 , loss : 0.001898, accuracy : 100.000000\n",
      "epoch : 62 , loss : 0.004304, accuracy : 100.000000\n",
      "epoch : 62 , loss : 0.118774, accuracy : 95.049505\n",
      "epoch : 63 , loss : 0.001756, accuracy : 100.000000\n",
      "epoch : 63 , loss : 0.004002, accuracy : 100.000000\n",
      "epoch : 63 , loss : 0.114570, accuracy : 95.049505\n",
      "epoch : 64 , loss : 0.001679, accuracy : 100.000000\n",
      "epoch : 64 , loss : 0.003937, accuracy : 100.000000\n",
      "epoch : 64 , loss : 0.110662, accuracy : 95.049505\n",
      "epoch : 65 , loss : 0.001657, accuracy : 100.000000\n",
      "epoch : 65 , loss : 0.003980, accuracy : 100.000000\n",
      "epoch : 65 , loss : 0.107062, accuracy : 95.049505\n",
      "epoch : 66 , loss : 0.001604, accuracy : 100.000000\n",
      "epoch : 66 , loss : 0.003848, accuracy : 100.000000\n",
      "epoch : 66 , loss : 0.103724, accuracy : 95.049505\n",
      "epoch : 67 , loss : 0.001534, accuracy : 100.000000\n",
      "epoch : 67 , loss : 0.003676, accuracy : 100.000000\n",
      "epoch : 67 , loss : 0.100629, accuracy : 95.049505\n",
      "epoch : 68 , loss : 0.001461, accuracy : 100.000000\n",
      "epoch : 68 , loss : 0.003495, accuracy : 100.000000\n",
      "epoch : 68 , loss : 0.097726, accuracy : 95.049505\n",
      "epoch : 69 , loss : 0.001384, accuracy : 100.000000\n",
      "epoch : 69 , loss : 0.003323, accuracy : 100.000000\n",
      "epoch : 69 , loss : 0.095000, accuracy : 96.039604\n",
      "epoch : 70 , loss : 0.001316, accuracy : 100.000000\n",
      "epoch : 70 , loss : 0.003259, accuracy : 100.000000\n",
      "epoch : 70 , loss : 0.092452, accuracy : 96.039604\n",
      "epoch : 71 , loss : 0.001247, accuracy : 100.000000\n",
      "epoch : 71 , loss : 0.003107, accuracy : 100.000000\n",
      "epoch : 71 , loss : 0.090293, accuracy : 96.039604\n",
      "epoch : 72 , loss : 0.001194, accuracy : 100.000000\n",
      "epoch : 72 , loss : 0.002835, accuracy : 100.000000\n",
      "epoch : 72 , loss : 0.088014, accuracy : 96.039604\n",
      "epoch : 73 , loss : 0.001031, accuracy : 100.000000\n",
      "epoch : 73 , loss : 0.002641, accuracy : 100.000000\n",
      "epoch : 73 , loss : 0.085845, accuracy : 96.039604\n",
      "epoch : 74 , loss : 0.000988, accuracy : 100.000000\n",
      "epoch : 74 , loss : 0.002541, accuracy : 100.000000\n",
      "epoch : 74 , loss : 0.083794, accuracy : 96.039604\n",
      "epoch : 75 , loss : 0.000958, accuracy : 100.000000\n",
      "epoch : 75 , loss : 0.002458, accuracy : 100.000000\n",
      "epoch : 75 , loss : 0.081852, accuracy : 96.039604\n",
      "epoch : 76 , loss : 0.000931, accuracy : 100.000000\n",
      "epoch : 76 , loss : 0.002381, accuracy : 100.000000\n",
      "epoch : 76 , loss : 0.080009, accuracy : 96.039604\n",
      "epoch : 77 , loss : 0.000907, accuracy : 100.000000\n",
      "epoch : 77 , loss : 0.002331, accuracy : 100.000000\n",
      "epoch : 77 , loss : 0.078262, accuracy : 96.039604\n",
      "epoch : 78 , loss : 0.000883, accuracy : 100.000000\n",
      "epoch : 78 , loss : 0.002278, accuracy : 100.000000\n",
      "epoch : 78 , loss : 0.076599, accuracy : 96.039604\n",
      "epoch : 79 , loss : 0.000860, accuracy : 100.000000\n",
      "epoch : 79 , loss : 0.002210, accuracy : 100.000000\n",
      "epoch : 79 , loss : 0.075017, accuracy : 96.039604\n",
      "epoch : 80 , loss : 0.000835, accuracy : 100.000000\n",
      "epoch : 80 , loss : 0.002143, accuracy : 100.000000\n",
      "epoch : 80 , loss : 0.073569, accuracy : 96.039604\n",
      "epoch : 81 , loss : 0.000970, accuracy : 100.000000\n",
      "epoch : 81 , loss : 0.002198, accuracy : 100.000000\n",
      "epoch : 81 , loss : 0.072152, accuracy : 96.039604\n",
      "epoch : 82 , loss : 0.000990, accuracy : 100.000000\n",
      "epoch : 82 , loss : 0.002311, accuracy : 100.000000\n",
      "epoch : 82 , loss : 0.070270, accuracy : 96.039604\n",
      "epoch : 83 , loss : 0.000862, accuracy : 100.000000\n",
      "epoch : 83 , loss : 0.002116, accuracy : 100.000000\n",
      "epoch : 83 , loss : 0.068956, accuracy : 96.039604\n",
      "epoch : 84 , loss : 0.000792, accuracy : 100.000000\n",
      "epoch : 84 , loss : 0.001982, accuracy : 100.000000\n",
      "epoch : 84 , loss : 0.067814, accuracy : 96.039604\n",
      "epoch : 85 , loss : 0.000940, accuracy : 100.000000\n",
      "epoch : 85 , loss : 0.002029, accuracy : 100.000000\n",
      "epoch : 85 , loss : 0.066633, accuracy : 96.039604\n",
      "epoch : 86 , loss : 0.000928, accuracy : 100.000000\n",
      "epoch : 86 , loss : 0.002097, accuracy : 100.000000\n",
      "epoch : 86 , loss : 0.065045, accuracy : 96.039604\n",
      "epoch : 87 , loss : 0.000812, accuracy : 100.000000\n",
      "epoch : 87 , loss : 0.001922, accuracy : 100.000000\n",
      "epoch : 87 , loss : 0.064026, accuracy : 96.039604\n",
      "epoch : 88 , loss : 0.000964, accuracy : 100.000000\n",
      "epoch : 88 , loss : 0.002067, accuracy : 100.000000\n",
      "epoch : 88 , loss : 0.062543, accuracy : 97.029703\n",
      "epoch : 89 , loss : 0.000819, accuracy : 100.000000\n",
      "epoch : 89 , loss : 0.001875, accuracy : 100.000000\n",
      "epoch : 89 , loss : 0.061578, accuracy : 96.039604\n",
      "epoch : 90 , loss : 0.000736, accuracy : 100.000000\n",
      "epoch : 90 , loss : 0.001753, accuracy : 100.000000\n",
      "epoch : 90 , loss : 0.060703, accuracy : 96.039604\n",
      "epoch : 91 , loss : 0.000888, accuracy : 100.000000\n",
      "epoch : 91 , loss : 0.001893, accuracy : 100.000000\n",
      "epoch : 91 , loss : 0.059365, accuracy : 97.029703\n",
      "epoch : 92 , loss : 0.000766, accuracy : 100.000000\n",
      "epoch : 92 , loss : 0.001748, accuracy : 100.000000\n",
      "epoch : 92 , loss : 0.058574, accuracy : 96.039604\n",
      "epoch : 93 , loss : 0.001006, accuracy : 100.000000\n",
      "epoch : 93 , loss : 0.002003, accuracy : 100.000000\n",
      "epoch : 93 , loss : 0.057253, accuracy : 97.029703\n",
      "epoch : 94 , loss : 0.000862, accuracy : 100.000000\n",
      "epoch : 94 , loss : 0.001795, accuracy : 100.000000\n",
      "epoch : 94 , loss : 0.056492, accuracy : 97.029703\n",
      "epoch : 95 , loss : 0.000740, accuracy : 100.000000\n",
      "epoch : 95 , loss : 0.001640, accuracy : 100.000000\n",
      "epoch : 95 , loss : 0.055798, accuracy : 96.039604\n",
      "epoch : 96 , loss : 0.000936, accuracy : 100.000000\n",
      "epoch : 96 , loss : 0.001853, accuracy : 100.000000\n",
      "epoch : 96 , loss : 0.054605, accuracy : 97.029703\n",
      "epoch : 97 , loss : 0.000823, accuracy : 100.000000\n",
      "epoch : 97 , loss : 0.001668, accuracy : 100.000000\n",
      "epoch : 97 , loss : 0.053985, accuracy : 97.029703\n",
      "epoch : 98 , loss : 0.000951, accuracy : 100.000000\n",
      "epoch : 98 , loss : 0.001819, accuracy : 100.000000\n",
      "epoch : 98 , loss : 0.052891, accuracy : 97.029703\n",
      "epoch : 99 , loss : 0.000844, accuracy : 100.000000\n",
      "epoch : 99 , loss : 0.001631, accuracy : 100.000000\n",
      "epoch : 99 , loss : 0.052284, accuracy : 97.029703\n",
      "epoch : 100 , loss : 0.000700, accuracy : 100.000000\n",
      "epoch : 100 , loss : 0.001466, accuracy : 100.000000\n",
      "epoch : 100 , loss : 0.051723, accuracy : 96.039604\n",
      "epoch : 101 , loss : 0.000815, accuracy : 100.000000\n",
      "epoch : 101 , loss : 0.001628, accuracy : 100.000000\n",
      "epoch : 101 , loss : 0.050719, accuracy : 97.029703\n",
      "epoch : 102 , loss : 0.000763, accuracy : 100.000000\n",
      "epoch : 102 , loss : 0.001484, accuracy : 100.000000\n",
      "epoch : 102 , loss : 0.050219, accuracy : 97.029703\n",
      "epoch : 103 , loss : 0.000828, accuracy : 100.000000\n",
      "epoch : 103 , loss : 0.001584, accuracy : 100.000000\n",
      "epoch : 103 , loss : 0.049254, accuracy : 97.029703\n",
      "epoch : 104 , loss : 0.000743, accuracy : 100.000000\n",
      "epoch : 104 , loss : 0.001427, accuracy : 100.000000\n",
      "epoch : 104 , loss : 0.048806, accuracy : 97.029703\n",
      "epoch : 105 , loss : 0.000796, accuracy : 100.000000\n",
      "epoch : 105 , loss : 0.001518, accuracy : 100.000000\n",
      "epoch : 105 , loss : 0.047902, accuracy : 97.029703\n",
      "epoch : 106 , loss : 0.000732, accuracy : 100.000000\n",
      "epoch : 106 , loss : 0.001374, accuracy : 100.000000\n",
      "epoch : 106 , loss : 0.047497, accuracy : 97.029703\n",
      "epoch : 107 , loss : 0.000753, accuracy : 100.000000\n",
      "epoch : 107 , loss : 0.001429, accuracy : 100.000000\n",
      "epoch : 107 , loss : 0.046639, accuracy : 97.029703\n",
      "epoch : 108 , loss : 0.000700, accuracy : 100.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 108 , loss : 0.001300, accuracy : 100.000000\n",
      "epoch : 108 , loss : 0.046266, accuracy : 96.039604\n",
      "epoch : 109 , loss : 0.000695, accuracy : 100.000000\n",
      "epoch : 109 , loss : 0.001340, accuracy : 100.000000\n",
      "epoch : 109 , loss : 0.045450, accuracy : 97.029703\n",
      "epoch : 110 , loss : 0.000667, accuracy : 100.000000\n",
      "epoch : 110 , loss : 0.001236, accuracy : 100.000000\n",
      "epoch : 110 , loss : 0.045104, accuracy : 96.039604\n",
      "epoch : 111 , loss : 0.000652, accuracy : 100.000000\n",
      "epoch : 111 , loss : 0.001269, accuracy : 100.000000\n",
      "epoch : 111 , loss : 0.044329, accuracy : 97.029703\n",
      "epoch : 112 , loss : 0.000644, accuracy : 100.000000\n",
      "epoch : 112 , loss : 0.001184, accuracy : 100.000000\n",
      "epoch : 112 , loss : 0.044012, accuracy : 96.039604\n",
      "epoch : 113 , loss : 0.000619, accuracy : 100.000000\n",
      "epoch : 113 , loss : 0.001203, accuracy : 100.000000\n",
      "epoch : 113 , loss : 0.043274, accuracy : 97.029703\n",
      "epoch : 114 , loss : 0.000617, accuracy : 100.000000\n",
      "epoch : 114 , loss : 0.001129, accuracy : 100.000000\n",
      "epoch : 114 , loss : 0.042978, accuracy : 96.039604\n",
      "epoch : 115 , loss : 0.000582, accuracy : 100.000000\n",
      "epoch : 115 , loss : 0.001144, accuracy : 100.000000\n",
      "epoch : 115 , loss : 0.042274, accuracy : 96.039604\n",
      "epoch : 116 , loss : 0.000596, accuracy : 100.000000\n",
      "epoch : 116 , loss : 0.001084, accuracy : 100.000000\n",
      "epoch : 116 , loss : 0.041997, accuracy : 96.039604\n",
      "epoch : 117 , loss : 0.000553, accuracy : 100.000000\n",
      "epoch : 117 , loss : 0.001086, accuracy : 100.000000\n",
      "epoch : 117 , loss : 0.041314, accuracy : 96.039604\n",
      "epoch : 118 , loss : 0.000581, accuracy : 100.000000\n",
      "epoch : 118 , loss : 0.001047, accuracy : 100.000000\n",
      "epoch : 118 , loss : 0.041056, accuracy : 96.039604\n",
      "epoch : 119 , loss : 0.000532, accuracy : 100.000000\n",
      "epoch : 119 , loss : 0.001047, accuracy : 100.000000\n",
      "epoch : 119 , loss : 0.040403, accuracy : 96.039604\n",
      "epoch : 120 , loss : 0.000567, accuracy : 100.000000\n",
      "epoch : 120 , loss : 0.001013, accuracy : 100.000000\n",
      "epoch : 120 , loss : 0.040162, accuracy : 96.039604\n",
      "epoch : 121 , loss : 0.000514, accuracy : 100.000000\n",
      "epoch : 121 , loss : 0.001008, accuracy : 100.000000\n",
      "epoch : 121 , loss : 0.039569, accuracy : 96.039604\n",
      "epoch : 122 , loss : 0.000584, accuracy : 100.000000\n",
      "epoch : 122 , loss : 0.001076, accuracy : 100.000000\n",
      "epoch : 122 , loss : 0.038964, accuracy : 96.039604\n",
      "epoch : 123 , loss : 0.000643, accuracy : 100.000000\n",
      "epoch : 123 , loss : 0.001044, accuracy : 100.000000\n",
      "epoch : 123 , loss : 0.038749, accuracy : 96.039604\n",
      "epoch : 124 , loss : 0.000531, accuracy : 100.000000\n",
      "epoch : 124 , loss : 0.000998, accuracy : 100.000000\n",
      "epoch : 124 , loss : 0.038157, accuracy : 96.039604\n",
      "epoch : 125 , loss : 0.000580, accuracy : 100.000000\n",
      "epoch : 125 , loss : 0.000964, accuracy : 100.000000\n",
      "epoch : 125 , loss : 0.037952, accuracy : 96.039604\n",
      "epoch : 126 , loss : 0.000487, accuracy : 100.000000\n",
      "epoch : 126 , loss : 0.000943, accuracy : 100.000000\n",
      "epoch : 126 , loss : 0.037421, accuracy : 96.039604\n",
      "epoch : 127 , loss : 0.000546, accuracy : 100.000000\n",
      "epoch : 127 , loss : 0.001003, accuracy : 100.000000\n",
      "epoch : 127 , loss : 0.036868, accuracy : 96.039604\n",
      "epoch : 128 , loss : 0.000610, accuracy : 100.000000\n",
      "epoch : 128 , loss : 0.000967, accuracy : 100.000000\n",
      "epoch : 128 , loss : 0.036692, accuracy : 96.039604\n",
      "epoch : 129 , loss : 0.000489, accuracy : 100.000000\n",
      "epoch : 129 , loss : 0.000928, accuracy : 100.000000\n",
      "epoch : 129 , loss : 0.036154, accuracy : 96.039604\n",
      "epoch : 130 , loss : 0.000530, accuracy : 100.000000\n",
      "epoch : 130 , loss : 0.000893, accuracy : 100.000000\n",
      "epoch : 130 , loss : 0.036000, accuracy : 96.039604\n",
      "epoch : 131 , loss : 0.000445, accuracy : 100.000000\n",
      "epoch : 131 , loss : 0.000875, accuracy : 100.000000\n",
      "epoch : 131 , loss : 0.035525, accuracy : 96.039604\n",
      "epoch : 132 , loss : 0.000502, accuracy : 100.000000\n",
      "epoch : 132 , loss : 0.000933, accuracy : 100.000000\n",
      "epoch : 132 , loss : 0.035017, accuracy : 96.039604\n",
      "epoch : 133 , loss : 0.000543, accuracy : 100.000000\n",
      "epoch : 133 , loss : 0.000891, accuracy : 100.000000\n",
      "epoch : 133 , loss : 0.034896, accuracy : 96.039604\n",
      "epoch : 134 , loss : 0.000450, accuracy : 100.000000\n",
      "epoch : 134 , loss : 0.000865, accuracy : 100.000000\n",
      "epoch : 134 , loss : 0.034459, accuracy : 96.039604\n",
      "epoch : 135 , loss : 0.000496, accuracy : 100.000000\n",
      "epoch : 135 , loss : 0.000915, accuracy : 100.000000\n",
      "epoch : 135 , loss : 0.033975, accuracy : 96.039604\n",
      "epoch : 136 , loss : 0.000519, accuracy : 100.000000\n",
      "epoch : 136 , loss : 0.000863, accuracy : 100.000000\n",
      "epoch : 136 , loss : 0.033883, accuracy : 96.039604\n",
      "epoch : 137 , loss : 0.000440, accuracy : 100.000000\n",
      "epoch : 137 , loss : 0.000842, accuracy : 100.000000\n",
      "epoch : 137 , loss : 0.033474, accuracy : 96.039604\n",
      "epoch : 138 , loss : 0.000484, accuracy : 100.000000\n",
      "epoch : 138 , loss : 0.000887, accuracy : 100.000000\n",
      "epoch : 138 , loss : 0.033022, accuracy : 96.039604\n",
      "epoch : 139 , loss : 0.000492, accuracy : 100.000000\n",
      "epoch : 139 , loss : 0.000822, accuracy : 100.000000\n",
      "epoch : 139 , loss : 0.032945, accuracy : 96.039604\n",
      "epoch : 140 , loss : 0.000422, accuracy : 100.000000\n",
      "epoch : 140 , loss : 0.000808, accuracy : 100.000000\n",
      "epoch : 140 , loss : 0.032560, accuracy : 96.039604\n",
      "epoch : 141 , loss : 0.000462, accuracy : 100.000000\n",
      "epoch : 141 , loss : 0.000856, accuracy : 100.000000\n",
      "epoch : 141 , loss : 0.032131, accuracy : 96.039604\n",
      "epoch : 142 , loss : 0.000434, accuracy : 100.000000\n",
      "epoch : 142 , loss : 0.000768, accuracy : 100.000000\n",
      "epoch : 142 , loss : 0.032074, accuracy : 96.039604\n",
      "epoch : 143 , loss : 0.000399, accuracy : 100.000000\n",
      "epoch : 143 , loss : 0.000775, accuracy : 100.000000\n",
      "epoch : 143 , loss : 0.031719, accuracy : 96.039604\n",
      "epoch : 144 , loss : 0.000437, accuracy : 100.000000\n",
      "epoch : 144 , loss : 0.000819, accuracy : 100.000000\n",
      "epoch : 144 , loss : 0.031377, accuracy : 96.039604\n",
      "epoch : 145 , loss : 0.000664, accuracy : 100.000000\n",
      "epoch : 145 , loss : 0.000823, accuracy : 100.000000\n",
      "epoch : 145 , loss : 0.031252, accuracy : 96.039604\n",
      "epoch : 146 , loss : 0.000361, accuracy : 100.000000\n",
      "epoch : 146 , loss : 0.000714, accuracy : 100.000000\n",
      "epoch : 146 , loss : 0.030916, accuracy : 96.039604\n",
      "epoch : 147 , loss : 0.000393, accuracy : 100.000000\n",
      "epoch : 147 , loss : 0.000760, accuracy : 100.000000\n",
      "epoch : 147 , loss : 0.030593, accuracy : 96.039604\n",
      "epoch : 148 , loss : 0.000425, accuracy : 100.000000\n",
      "epoch : 148 , loss : 0.000782, accuracy : 100.000000\n",
      "epoch : 148 , loss : 0.030208, accuracy : 96.039604\n",
      "epoch : 149 , loss : 0.000373, accuracy : 100.000000\n",
      "epoch : 149 , loss : 0.000692, accuracy : 100.000000\n",
      "epoch : 149 , loss : 0.030196, accuracy : 96.039604\n",
      "epoch : 150 , loss : 0.000372, accuracy : 100.000000\n",
      "epoch : 150 , loss : 0.000719, accuracy : 100.000000\n",
      "epoch : 150 , loss : 0.029889, accuracy : 96.039604\n",
      "epoch : 151 , loss : 0.000391, accuracy : 100.000000\n",
      "epoch : 151 , loss : 0.000733, accuracy : 100.000000\n",
      "epoch : 151 , loss : 0.029595, accuracy : 96.039604\n",
      "epoch : 152 , loss : 0.000601, accuracy : 100.000000\n",
      "epoch : 152 , loss : 0.000750, accuracy : 100.000000\n",
      "epoch : 152 , loss : 0.029501, accuracy : 96.039604\n",
      "epoch : 153 , loss : 0.000320, accuracy : 100.000000\n",
      "epoch : 153 , loss : 0.000641, accuracy : 100.000000\n",
      "epoch : 153 , loss : 0.029205, accuracy : 96.039604\n",
      "epoch : 154 , loss : 0.000346, accuracy : 100.000000\n",
      "epoch : 154 , loss : 0.000670, accuracy : 100.000000\n",
      "epoch : 154 , loss : 0.028923, accuracy : 96.039604\n",
      "epoch : 155 , loss : 0.000374, accuracy : 100.000000\n",
      "epoch : 155 , loss : 0.000701, accuracy : 100.000000\n",
      "epoch : 155 , loss : 0.028650, accuracy : 96.039604\n",
      "epoch : 156 , loss : 0.000584, accuracy : 100.000000\n",
      "epoch : 156 , loss : 0.000717, accuracy : 100.000000\n",
      "epoch : 156 , loss : 0.028570, accuracy : 96.039604\n",
      "epoch : 157 , loss : 0.000306, accuracy : 100.000000\n",
      "epoch : 157 , loss : 0.000618, accuracy : 100.000000\n",
      "epoch : 157 , loss : 0.028296, accuracy : 96.039604\n",
      "epoch : 158 , loss : 0.000330, accuracy : 100.000000\n",
      "epoch : 158 , loss : 0.000643, accuracy : 100.000000\n",
      "epoch : 158 , loss : 0.028035, accuracy : 96.039604\n",
      "epoch : 159 , loss : 0.000354, accuracy : 100.000000\n",
      "epoch : 159 , loss : 0.000665, accuracy : 100.000000\n",
      "epoch : 159 , loss : 0.027781, accuracy : 96.039604\n",
      "epoch : 160 , loss : 0.000565, accuracy : 100.000000\n",
      "epoch : 160 , loss : 0.000687, accuracy : 100.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 160 , loss : 0.027712, accuracy : 96.039604\n",
      "epoch : 161 , loss : 0.000289, accuracy : 100.000000\n",
      "epoch : 161 , loss : 0.000577, accuracy : 100.000000\n",
      "epoch : 161 , loss : 0.027458, accuracy : 96.039604\n",
      "epoch : 162 , loss : 0.000312, accuracy : 100.000000\n",
      "epoch : 162 , loss : 0.000611, accuracy : 100.000000\n",
      "epoch : 162 , loss : 0.027213, accuracy : 96.039604\n",
      "epoch : 163 , loss : 0.000340, accuracy : 100.000000\n",
      "epoch : 163 , loss : 0.000636, accuracy : 100.000000\n",
      "epoch : 163 , loss : 0.026976, accuracy : 96.039604\n",
      "epoch : 164 , loss : 0.000363, accuracy : 100.000000\n",
      "epoch : 164 , loss : 0.000655, accuracy : 100.000000\n",
      "epoch : 164 , loss : 0.026739, accuracy : 96.039604\n",
      "epoch : 165 , loss : 0.000559, accuracy : 100.000000\n",
      "epoch : 165 , loss : 0.000667, accuracy : 100.000000\n",
      "epoch : 165 , loss : 0.026702, accuracy : 96.039604\n",
      "epoch : 166 , loss : 0.000294, accuracy : 100.000000\n",
      "epoch : 166 , loss : 0.000569, accuracy : 100.000000\n",
      "epoch : 166 , loss : 0.026470, accuracy : 96.039604\n",
      "epoch : 167 , loss : 0.000306, accuracy : 100.000000\n",
      "epoch : 167 , loss : 0.000588, accuracy : 100.000000\n",
      "epoch : 167 , loss : 0.026245, accuracy : 96.039604\n",
      "epoch : 168 , loss : 0.000333, accuracy : 100.000000\n",
      "epoch : 168 , loss : 0.000613, accuracy : 100.000000\n",
      "epoch : 168 , loss : 0.026026, accuracy : 96.039604\n",
      "epoch : 169 , loss : 0.000359, accuracy : 100.000000\n",
      "epoch : 169 , loss : 0.000631, accuracy : 100.000000\n",
      "epoch : 169 , loss : 0.025807, accuracy : 96.039604\n",
      "epoch : 170 , loss : 0.000548, accuracy : 100.000000\n",
      "epoch : 170 , loss : 0.000642, accuracy : 100.000000\n",
      "epoch : 170 , loss : 0.025783, accuracy : 96.039604\n",
      "epoch : 171 , loss : 0.000286, accuracy : 100.000000\n",
      "epoch : 171 , loss : 0.000546, accuracy : 100.000000\n",
      "epoch : 171 , loss : 0.025569, accuracy : 96.039604\n",
      "epoch : 172 , loss : 0.000299, accuracy : 100.000000\n",
      "epoch : 172 , loss : 0.000565, accuracy : 100.000000\n",
      "epoch : 172 , loss : 0.025360, accuracy : 96.039604\n",
      "epoch : 173 , loss : 0.000329, accuracy : 100.000000\n",
      "epoch : 173 , loss : 0.000596, accuracy : 100.000000\n",
      "epoch : 173 , loss : 0.025166, accuracy : 96.039604\n",
      "epoch : 174 , loss : 0.000358, accuracy : 100.000000\n",
      "epoch : 174 , loss : 0.000617, accuracy : 100.000000\n",
      "epoch : 174 , loss : 0.024972, accuracy : 96.039604\n",
      "epoch : 175 , loss : 0.000556, accuracy : 100.000000\n",
      "epoch : 175 , loss : 0.000623, accuracy : 100.000000\n",
      "epoch : 175 , loss : 0.024956, accuracy : 96.039604\n",
      "epoch : 176 , loss : 0.000277, accuracy : 100.000000\n",
      "epoch : 176 , loss : 0.000524, accuracy : 100.000000\n",
      "epoch : 176 , loss : 0.024756, accuracy : 96.039604\n",
      "epoch : 177 , loss : 0.000292, accuracy : 100.000000\n",
      "epoch : 177 , loss : 0.000543, accuracy : 100.000000\n",
      "epoch : 177 , loss : 0.024562, accuracy : 96.039604\n",
      "epoch : 178 , loss : 0.000320, accuracy : 100.000000\n",
      "epoch : 178 , loss : 0.000573, accuracy : 100.000000\n",
      "epoch : 178 , loss : 0.024381, accuracy : 96.039604\n",
      "epoch : 179 , loss : 0.000351, accuracy : 100.000000\n",
      "epoch : 179 , loss : 0.000596, accuracy : 100.000000\n",
      "epoch : 179 , loss : 0.024203, accuracy : 96.039604\n",
      "epoch : 180 , loss : 0.000551, accuracy : 100.000000\n",
      "epoch : 180 , loss : 0.000602, accuracy : 100.000000\n",
      "epoch : 180 , loss : 0.024189, accuracy : 96.039604\n",
      "epoch : 181 , loss : 0.000270, accuracy : 100.000000\n",
      "epoch : 181 , loss : 0.000503, accuracy : 100.000000\n",
      "epoch : 181 , loss : 0.024002, accuracy : 96.039604\n",
      "epoch : 182 , loss : 0.000286, accuracy : 100.000000\n",
      "epoch : 182 , loss : 0.000529, accuracy : 100.000000\n",
      "epoch : 182 , loss : 0.023828, accuracy : 96.039604\n",
      "epoch : 183 , loss : 0.000320, accuracy : 100.000000\n",
      "epoch : 183 , loss : 0.000558, accuracy : 100.000000\n",
      "epoch : 183 , loss : 0.023660, accuracy : 96.039604\n",
      "epoch : 184 , loss : 0.000345, accuracy : 100.000000\n",
      "epoch : 184 , loss : 0.000576, accuracy : 100.000000\n",
      "epoch : 184 , loss : 0.023495, accuracy : 96.039604\n",
      "epoch : 185 , loss : 0.000365, accuracy : 100.000000\n",
      "epoch : 185 , loss : 0.000588, accuracy : 100.000000\n",
      "epoch : 185 , loss : 0.023330, accuracy : 96.039604\n",
      "epoch : 186 , loss : 0.000552, accuracy : 100.000000\n",
      "epoch : 186 , loss : 0.000584, accuracy : 100.000000\n",
      "epoch : 186 , loss : 0.023331, accuracy : 96.039604\n",
      "epoch : 187 , loss : 0.000272, accuracy : 100.000000\n",
      "epoch : 187 , loss : 0.000490, accuracy : 100.000000\n",
      "epoch : 187 , loss : 0.023161, accuracy : 96.039604\n",
      "epoch : 188 , loss : 0.000291, accuracy : 100.000000\n",
      "epoch : 188 , loss : 0.000513, accuracy : 100.000000\n",
      "epoch : 188 , loss : 0.022994, accuracy : 96.039604\n",
      "epoch : 189 , loss : 0.000314, accuracy : 100.000000\n",
      "epoch : 189 , loss : 0.000537, accuracy : 100.000000\n",
      "epoch : 189 , loss : 0.022838, accuracy : 96.039604\n",
      "epoch : 190 , loss : 0.000343, accuracy : 100.000000\n",
      "epoch : 190 , loss : 0.000556, accuracy : 100.000000\n",
      "epoch : 190 , loss : 0.022686, accuracy : 96.039604\n",
      "epoch : 191 , loss : 0.000362, accuracy : 100.000000\n",
      "epoch : 191 , loss : 0.000568, accuracy : 100.000000\n",
      "epoch : 191 , loss : 0.022536, accuracy : 96.039604\n",
      "epoch : 192 , loss : 0.000547, accuracy : 100.000000\n",
      "epoch : 192 , loss : 0.000562, accuracy : 100.000000\n",
      "epoch : 192 , loss : 0.022541, accuracy : 96.039604\n",
      "epoch : 193 , loss : 0.000267, accuracy : 100.000000\n",
      "epoch : 193 , loss : 0.000473, accuracy : 100.000000\n",
      "epoch : 193 , loss : 0.022383, accuracy : 96.039604\n",
      "epoch : 194 , loss : 0.000286, accuracy : 100.000000\n",
      "epoch : 194 , loss : 0.000498, accuracy : 100.000000\n",
      "epoch : 194 , loss : 0.022236, accuracy : 96.039604\n",
      "epoch : 195 , loss : 0.000314, accuracy : 100.000000\n",
      "epoch : 195 , loss : 0.000519, accuracy : 100.000000\n",
      "epoch : 195 , loss : 0.022092, accuracy : 96.039604\n",
      "epoch : 196 , loss : 0.000338, accuracy : 100.000000\n",
      "epoch : 196 , loss : 0.000535, accuracy : 100.000000\n",
      "epoch : 196 , loss : 0.021951, accuracy : 96.039604\n",
      "epoch : 197 , loss : 0.000357, accuracy : 100.000000\n",
      "epoch : 197 , loss : 0.000547, accuracy : 100.000000\n",
      "epoch : 197 , loss : 0.021813, accuracy : 96.039604\n",
      "epoch : 198 , loss : 0.000373, accuracy : 100.000000\n",
      "epoch : 198 , loss : 0.000555, accuracy : 100.000000\n",
      "epoch : 198 , loss : 0.021675, accuracy : 96.039604\n",
      "epoch : 199 , loss : 0.000500, accuracy : 100.000000\n",
      "epoch : 199 , loss : 0.000548, accuracy : 100.000000\n",
      "epoch : 199 , loss : 0.021664, accuracy : 96.039604\n",
      "epoch : 200 , loss : 0.000293, accuracy : 100.000000\n",
      "epoch : 200 , loss : 0.000485, accuracy : 100.000000\n",
      "epoch : 200 , loss : 0.021527, accuracy : 96.039604\n"
     ]
    }
   ],
   "source": [
    "loss_log = []\n",
    "\n",
    "for epoch in range(0,200):\n",
    "    running_loss = 0.\n",
    "    correct = 0\n",
    "    for i in range(0,train_len):\n",
    "        input = train_feature[i,:]\n",
    "        label = int(dic[train_label[i][:-1]])\n",
    "\n",
    "        input = torch.FloatTensor(input).view(1,-1)\n",
    "        label = torch.LongTensor([label])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #     print(label.shape)\n",
    "        output = nets(input)\n",
    "        #     print(output.shape, label.shape)\n",
    "        loss = criterion(output, label)\n",
    "        running_loss += loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        prediction = torch.max(output.data,1)[1]\n",
    "\n",
    "        correct += 1 if prediction == label else 0\n",
    "#         print(prediction, label)\n",
    "        if i%50 == 0:\n",
    "            print('epoch : %d , loss : %.6f, accuracy : %.6f' % (epoch+1, loss.data[0], 100 * 1.0 * correct/ (i+1)))\n",
    "    loss_log.append(running_loss/(1.0*train_len))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ea87dd358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction : 1 real label : 1\n",
      "accuracy : 100.0000\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 100.0000\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 100.0000\n",
      "prediction : 0 real label : 0\n",
      "accuracy : 100.0000\n",
      "prediction : 0 real label : 0\n",
      "accuracy : 100.0000\n",
      "prediction : 2 real label : 1\n",
      "accuracy : 83.3333\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 85.7143\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 87.5000\n",
      "prediction : 0 real label : 0\n",
      "accuracy : 88.8889\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 90.0000\n",
      "prediction : 0 real label : 0\n",
      "accuracy : 90.9091\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 91.6667\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 92.3077\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 92.8571\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 93.3333\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 93.7500\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 94.1176\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 94.4444\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 94.7368\n",
      "prediction : 0 real label : 0\n",
      "accuracy : 95.0000\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 95.2381\n",
      "prediction : 0 real label : 0\n",
      "accuracy : 95.4545\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 95.6522\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 95.8333\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 96.0000\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 96.1538\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 96.2963\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 96.4286\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 96.5517\n",
      "prediction : 0 real label : 0\n",
      "accuracy : 96.6667\n",
      "prediction : 0 real label : 0\n",
      "accuracy : 96.7742\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 96.8750\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 96.9697\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 97.0588\n",
      "prediction : 2 real label : 1\n",
      "accuracy : 94.2857\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 94.4444\n",
      "prediction : 0 real label : 0\n",
      "accuracy : 94.5946\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 94.7368\n",
      "prediction : 2 real label : 1\n",
      "accuracy : 92.3077\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 92.5000\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 92.6829\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 92.8571\n",
      "prediction : 1 real label : 1\n",
      "accuracy : 93.0233\n",
      "prediction : 2 real label : 2\n",
      "accuracy : 93.1818\n",
      "prediction : 0 real label : 0\n",
      "accuracy : 93.3333\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "epochs = [i for i in range(0,200)]\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.plot(epochs,loss_log)\n",
    "plt.show()\n",
    "\n",
    "for i in range(0, len(feature) - train_len):\n",
    "    input = test_feature[i]\n",
    "    label = int(dic[test_label[i][:-1]])\n",
    "\n",
    "    \n",
    "    input= torch.FloatTensor(input).view(1,-1)\n",
    "    \n",
    "    output = nets(input)\n",
    "    \n",
    "    prediction = torch.max(output.data,1)[1]\n",
    "    \n",
    "    correct += 1 if prediction == label else 0\n",
    "    \n",
    "    print('prediction : {} real label : {}'.format(prediction.item(), label))\n",
    "    print('accuracy : %.4f' % (100 * 1.0 * correct/(i+1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
